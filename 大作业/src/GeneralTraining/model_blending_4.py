import os
import time
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import roc_auc_score, f1_score, log_loss, roc_curve, precision_recall_curve
from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset


# ==========================================
# 1. é…ç½®å‚æ•° - å®éªŒæ”¶å‰²æ¨¡å¼
# ==========================================
class Config:
    # å­æ¨¡å‹ç»“æœè·¯å¾„
    MODEL_DIRS = {
        "LightGBM": "../../result/lgbm_result/lgbm_preds.csv",
        "XGBoost": "../../result/xgb_result/xgb_preds.csv",
        "CatBoost": "../../result/cat_result/cat_preds.csv",
        "TabNet": "../../result/tabnet_result/tabnet_preds.csv",
    }
    LABEL_PATH = "../../data/y_test_final.csv"

    # æœ€ç»ˆè¾“å‡ºæ–‡ä»¶å¤¹
    FINAL_DIR = "../../result/blending_result"

    # èåˆæƒé‡ (æ ¹æ®å„æ¨¡å‹å•æ¨¡è¡¨ç°è°ƒæ•´ï¼ŒCatBoostæœ€é«˜)
    WEIGHTS = {"LightGBM": 0.25, "XGBoost": 0.15, "CatBoost": 0.40, "TabNet": 0.20}


os.makedirs(Config.FINAL_DIR, exist_ok=True)


# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ï¼šèåˆä¸è¯„ä¼°
# ==========================================
def run_final_blending():
    print("ğŸ”® æ­£åœ¨å¯åŠ¨å…¨æ¨¡å‹æ ‡å‡†åŒ–èåˆæµç¨‹...")

    # 1. è½½å…¥çœŸå®æ ‡ç­¾
    y_test = pd.read_csv(Config.LABEL_PATH).values.ravel()

    # 2. è½½å…¥å„æ¨¡å‹é¢„æµ‹æ¦‚ç‡å¹¶æ„å»º DataFrame
    all_probs = pd.DataFrame()
    for name, path in Config.MODEL_DIRS.items():
        if os.path.exists(path):
            all_probs[name] = pd.read_csv(path)["prob"].values
        else:
            print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° {name} çš„é¢„æµ‹æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")

    # 3. è®¡ç®— Blending æ¦‚ç‡
    final_prob = np.zeros_like(y_test, dtype=float)
    for name, weight in Config.WEIGHTS.items():
        final_prob += all_probs[name] * weight
    all_probs["Blending_Ensemble"] = final_prob

    # 4. æŒ‡æ ‡ç»Ÿè®¡
    metrics_list = []
    for col in all_probs.columns:
        prob = all_probs[col]
        auc = roc_auc_score(y_test, prob)
        loss = log_loss(y_test, prob)
        prec, rec, _ = precision_recall_curve(y_test, prob)
        f1 = 2 * (prec * rec) / (prec + rec + 1e-8)
        metrics_list.append(
            {"Model": col, "AUC": round(auc, 6), "LogLoss": round(loss, 6), "Max_F1": round(np.max(f1), 6)}
        )

    df_metrics = pd.DataFrame(metrics_list).sort_values(by="AUC", ascending=False)

    # ä¿å­˜æŒ‡æ ‡è¡¨æ ¼
    df_metrics.to_csv(os.path.join(Config.FINAL_DIR, "overall_metrics.csv"), index=False)
    print("\n" + df_metrics.to_markdown(index=False))

    # 5. å¯è§†åŒ– Aï¼šå…¨æ¨¡å‹ ROC å¯¹æ¯”å›¾ (å¸¦å±€éƒ¨æ”¾å¤§)
    plot_comparison_roc(y_test, all_probs)

    # 6. å¯è§†åŒ– Bï¼šæ¨¡å‹ç›¸å…³æ€§çƒ­åŠ›å›¾ (å†—ä½™åˆ†æ)
    plot_correlation(all_probs.drop(columns="Blending_Ensemble"))

    # 7. ä¿å­˜æœ€ç»ˆé¢„æµ‹ç»“æœ
    all_probs[["Blending_Ensemble"]].to_csv(
        os.path.join(Config.FINAL_DIR, "final_ensemble_preds.csv"), index=False
    )
    print(f"\nâœ… ç»ˆææŠ¥å‘Šå·²ç”Ÿæˆè‡³: {Config.FINAL_DIR}")


# ==========================================
# 3. ç»˜å›¾å‡½æ•°åº“
# ==========================================
def plot_comparison_roc(y_test, df_probs):
    plt.figure(figsize=(12, 8))
    ax = plt.gca()

    colors = sns.color_palette("husl", len(df_probs.columns))

    for i, col in enumerate(df_probs.columns):
        fpr, tpr, _ = roc_curve(y_test, df_probs[col])
        lw = 3 if col == "Blending_Ensemble" else 1.5
        alpha = 1.0 if col == "Blending_Ensemble" else 0.7
        linestyle = "-" if col == "Blending_Ensemble" else "--"
        color = "black" if col == "Blending_Ensemble" else colors[i]

        plt.plot(
            fpr,
            tpr,
            label=f"{col} (AUC={roc_auc_score(y_test, df_probs[col]):.4f})",
            lw=lw,
            alpha=alpha,
            linestyle=linestyle,
            color=color,
        )

    plt.plot([0, 1], [0, 1], "k--", alpha=0.2)

    # å±€éƒ¨æ”¾å¤§
    axins = inset_axes(ax, width="40%", height="40%", loc="lower right", borderpad=3)
    for i, col in enumerate(df_probs.columns):
        fpr, tpr, _ = roc_curve(y_test, df_probs[col])
        color = "black" if col == "Blending_Ensemble" else colors[i]
        axins.plot(fpr, tpr, color=color, lw=2 if col == "Blending_Ensemble" else 1)

    axins.set_xlim(0.1, 0.3)
    axins.set_ylim(0.6, 0.8)
    mark_inset(ax, axins, loc1=2, loc2=4, fc="none", ec="0.5", ls="--")

    plt.title("Final Model Comparison: ROC Curves", fontsize=15)
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.legend(loc="upper left", bbox_to_anchor=(1, 1))
    plt.grid(alpha=0.2)
    plt.savefig(os.path.join(Config.FINAL_DIR, "final_roc_comparison.png"), dpi=300, bbox_inches="tight")
    plt.close()


def plot_correlation(df_probs):
    plt.figure(figsize=(10, 8))
    corr = df_probs.corr()
    sns.heatmap(corr, annot=True, cmap="RdYlGn", fmt=".4f", center=0.95)
    plt.title("Model Prediction Correlation (Redundancy Analysis)", fontsize=15)
    plt.savefig(os.path.join(Config.FINAL_DIR, "model_correlation_heatmap.png"), dpi=300, bbox_inches="tight")
    plt.close()


if __name__ == "__main__":
    run_final_blending()
