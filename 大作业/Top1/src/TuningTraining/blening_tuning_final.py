import os
import time
import pandas as pd
import numpy as np
import optuna
from sklearn.metrics import roc_auc_score, log_loss, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns


# ==========================================
# 1. é…ç½®å‚æ•°
# ==========================================
class Config:
    # è°ƒå‚åçš„å››ä¸ªæ¨¡å‹é¢„æµ‹ç»“æœè·¯å¾„
    MODEL_PREDS = {
        "CatBoost": "../../result/cat_tuning_result/cat_tuned_preds.csv",
        "LightGBM": "../../result/lgbm_tuning_result/lgbm_tuned_preds.csv",
        "XGBoost": "../../result/xgb_tuning_result/xgb_tuned_preds.csv",
        "TabNet": "../../result/tabnet_result/tabnet_preds.csv",
    }

    # æ ‡ç­¾è·¯å¾„
    LABEL_PATH = "../../data/y_test_final.csv"

    # è¾“å‡ºç›®å½•
    OUTPUT_DIR = "../../result/blending_tuning_result"
    N_TRIALS = 150  # ç¨å¾®å¢åŠ æœç´¢æ¬¡æ•°ä»¥è·å¾—æ›´ç²¾ç»†çš„æƒé‡

    REPORT_TXT = os.path.join(OUTPUT_DIR, "blending_report.txt")
    WEIGHTS_PNG = os.path.join(OUTPUT_DIR, "optimized_weights_pie.png")
    CORR_PNG = os.path.join(OUTPUT_DIR, "model_correlation_heatmap.png")


os.makedirs(Config.OUTPUT_DIR, exist_ok=True)


# ==========================================
# 2. è½½å…¥æ•°æ®ä¸å¯¹é½
# ==========================================
def load_data():
    # è½½å…¥çœŸå®æ ‡ç­¾ (å–æœ€åä¸€åˆ—)
    df_label = pd.read_csv(Config.LABEL_PATH)
    y_true = df_label.iloc[:, -1].values.ravel()

    # è½½å…¥å„æ¨¡å‹é¢„æµ‹æ¦‚ç‡
    preds_dict = {}
    for name, path in Config.MODEL_PREDS.items():
        if os.path.exists(path):
            # ç¡®ä¿è¯»å–çš„æ˜¯ prob åˆ—
            preds_dict[name] = pd.read_csv(path)["prob"].values
        else:
            print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° {name} çš„é¢„æµ‹æ–‡ä»¶ï¼Œè·³è¿‡è¯¥æ¨¡å‹ã€‚")

    return y_true, preds_dict


# ==========================================
# 3. èµ„äº§ç”Ÿæˆé€»è¾‘
# ==========================================
def generate_assets(y_true, preds_dict, best_weights, final_auc):
    # 1. è®¡ç®—å„æ¨¡å‹ç‹¬ç«‹ AUC
    individual_aucs = {name: roc_auc_score(y_true, prob) for name, prob in preds_dict.items()}

    # 2. ç”Ÿæˆç›¸å…³æ€§çƒ­å›¾
    # èåˆåŸåˆ™ï¼šæ¨¡å‹é—´çš„ç›¸å…³æ€§è¶Šä½ï¼Œèåˆæ”¶ç›Šè¶Šé«˜
    df_corr = pd.DataFrame(preds_dict).corr()
    plt.figure(figsize=(10, 8))
    sns.heatmap(df_corr, annot=True, cmap="coolwarm", fmt=".4f")
    plt.title("Model Prediction Correlation")
    plt.savefig(Config.CORR_PNG, dpi=300)
    plt.close()

    # 3. å†™å…¥å®éªŒæŠ¥å‘Š
    with open(Config.REPORT_TXT, "w", encoding="utf-8") as f:
        f.write("=" * 45 + "\n")
        f.write("      èµ›é©¬é¢„æµ‹å…¨æ¨¡å‹èåˆ(Blending)æŠ¥å‘Š\n")
        f.write("=" * 45 + "\n")
        f.write(f"å®Œæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        f.write("ğŸ“ˆ [å•ä¸€æ¨¡å‹è¡¨ç°å¯¹æ¯” (AUC)]\n")
        for name, score in individual_aucs.items():
            f.write(f" - {name:10}: {score:.6f}\n")

        f.write(f"\nğŸš€ [èåˆåè¡¨ç°]\n")
        f.write(f" - Final Blended AUC: {final_auc:.6f}\n")
        improvement = final_auc - max(individual_aucs.values())
        f.write(f" - ç›¸æ¯”æœ€å¼ºå•æ¨¡å‹æå‡: {improvement:.6f}\n\n")

        f.write("âš–ï¸ [æœ€ä¼˜æƒé‡åˆ†é…]\n")
        for name, w in best_weights.items():
            f.write(f" - {name:10}: {w*100:.2f}%\n")

    # 4. æƒé‡å æ¯”é¥¼å›¾
    plt.figure(figsize=(10, 6))
    names = list(best_weights.keys())
    vals = list(best_weights.values())
    plt.pie(
        vals, labels=names, autopct="%1.1f%%", startangle=140, colors=sns.color_palette("viridis", len(names))
    )
    plt.title("Optimized Model Contribution")
    plt.savefig(Config.WEIGHTS_PNG, dpi=300)
    plt.close()


# ==========================================
# 4. ä¸»ç¨‹åº
# ==========================================
def objective(trial, y_true, preds_dict):
    weights = {name: trial.suggest_float(name, 0.0, 1.0) for name in preds_dict.keys()}

    total_w = sum(weights.values())
    if total_w == 0:
        return 0

    blended_prob = sum(prob * (weights[name] / total_w) for name, prob in preds_dict.items())
    return roc_auc_score(y_true, blended_prob)


def main():
    print(f"[{time.strftime('%H:%M:%S')}] åŠ è½½é¢„æµ‹ç»“æœä¸­...")
    y_true, preds_dict = load_data()

    if len(preds_dict) < 2:
        print("âŒ é”™è¯¯: éœ€è¦è‡³å°‘ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœæ‰èƒ½è¿›è¡Œèåˆã€‚")
        return

    # 1. Optuna æƒé‡å¯»ä¼˜
    study = optuna.create_study(direction="maximize")
    study.optimize(lambda trial: objective(trial, y_true, preds_dict), n_trials=Config.N_TRIALS)

    # 2. å½’ä¸€åŒ–æƒé‡
    best_raw = study.best_params
    total_w = sum(best_raw.values())
    best_weights = {k: v / total_w for k, v in best_raw.items()}

    print(f"\nâœ… å¯»ä¼˜å®Œæˆ! èåˆ AUC: {study.best_value:.6f}")

    # 3. è®¡ç®—å¹¶ä¿å­˜æœ€ç»ˆé¢„æµ‹æ¦‚ç‡
    final_prob = sum(preds_dict[name] * best_weights[name] for name in preds_dict.keys())
    pd.DataFrame({"prob": final_prob}).to_csv(
        os.path.join(Config.OUTPUT_DIR, "final_blended_preds.csv"), index=False
    )

    # 4. ç”Ÿæˆå›¾è¡¨ä¸æŠ¥å‘Š
    generate_assets(y_true, preds_dict, best_weights, study.best_value)

    print(f"\nâœ¨ èåˆèµ„äº§å·²å½’æ¡£è‡³: {Config.RESULT_DIR}")
    print(f"ğŸ“Š è¯·æŸ¥çœ‹ç›¸å…³æ€§çƒ­å›¾: {Config.CORR_PNG}")


if __name__ == "__main__":
    main()
