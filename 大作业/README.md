- 中山 06
- 中京 07
- 阪神 09
- 东京 05
- 京都 08
- 福岛 03
- 新潟 04
- 札幌 01
- 函館 02
- 小倉 10

- 竞赛查询网址：https://db.netkeiba.com/race/202502010701/
- 格式解析：
  1. 2025：年
  2. 02：竞赛场地
  3. 01：第几回比赛
  4. 07：第几日
  5. 01：当日第几场比赛

### 数据获取

- 爬虫
- netbeika
- 多次被封
- 10 年的 jra 中央赛马数据
- 共 33002 场比赛，54171 匹赛马，438536 条完赛数据
- 包含马性别、年龄、赛场场地、时间、距离、人气、赔率...
- 存储到 csv 文件中

### 数据导入数据库

- mysql
- 三个表：horse，race，horse_race

### 数据清洗

- 删除乱码数据（1400 条左右）
- 删除障碍赛数据（接近 1000 条）
- 删除赔率异常数据（接近 1000 条 >550）

### 数据预处理与特征工程

这个脚本围绕赛马数据特征工程完成了全流程的数据预处理，核心目标是适配 “预测马匹进入前 3 名” 的分类任务（AUC 0.85 目标），同时解决过拟合、数据泄露、时序逻辑等关键问题。以下是分模块的预处理细节拆解：
一、数据读取与基础时序排序（预处理前置）
核心操作：
通过 SQLAlchemy 连接 MySQL 数据库，读取核心表（horse_race_cleaned/horse/race）的关联数据，包含赛事、马匹、骑手、场地等基础特征。
按 race_id（赛事 ID）+gate_no（闸位）排序并重置索引，确保数据符合赛事发生的时间逻辑（后续时序切分、历史特征计算的基础）。
二、稳健性预处理（异常值 + 缺失值治理）
目标：消除极端值和缺失值对模型的干扰，保证数据质量。

1. 异常值限制（Clip）
   对 win_odds（获胜赔率）：限制范围[1.0, 500.0]，避免极端赔率（如 1000+）主导特征分布；
   对 popularity（人气值）：限制范围[1, 20]，过滤异常人气值。
2. 缺失值智能填充
   按 “先分组、后全局” 的优先级填充，避免简单填充的偏差：
   数值列（racing_age/gate_no/jockey_weight 等）：优先用同 race_id（同场赛事）的均值填充，若仍有缺失则用全局均值；
   类别列（gender/venue_type/direction 等）：填充列的众数（最常见值），保证类别特征的合理性。
   三、组内相对特征构建（核心改进：解决过拟合）
   目标：消除不同赛事的系统差异（如不同场次的赔率基准、负重基准不同），聚焦单场赛事内的相对竞争力。按 race_id 分组计算以下特征：
   odds_vs_mean：单马赔率 / 本场平均赔率（衡量该马在本场的赔率相对水平）；
   pop_vs_mean：单马人气 / 本场平均人气（相对人气水平）；
   weight_vs_mean：单马骑手负重 - 本场平均负重（负重偏离度）；
   race_odds_rank：单马在本场的赔率排名（1 = 本场最被看好）。
   四、高效历史特征计算（避免数据泄露 + 提升效率）
   目标：挖掘马匹的历史参赛表现，同时严格规避 “未来数据泄露”（即用当前场数据计算历史特征）。
   前置：按 horse_name+race_id 排序，保证单匹马的参赛时序。
   核心操作（Expanding Window 滑动窗口，替代循环提升效率）：
   history_race_count：该马历史参赛场数（不含当前场，通过索引计数）；
   history_top3_count：历史前 3 名次数（用 shift(1)将当前场结果后移，仅计算过去场次，避免泄露）；
   history_avg_rank：历史平均排名（同样通过 shift(1)确保仅用历史排名）；
   history_win_rate：历史胜率（Top3 率 = 历史前 3 次数 / 历史参赛场数）。
   五、交互特征与编码（增强特征表达）
3. 构造交互 / 变换特征
   win_odds_log：赔率的对数变换（log1p），将非线性分布的赔率转为更平滑的分布；
   weight_per_dist：骑手负重 /（赛事距离 / 1000），构建 “负重 - 距离” 交互特征，反映单位距离的负重压力。
4. 类别特征编码
   对性别（gender）、场地类型（venue_type）、赛道方向（direction）、天气（weather）、赛道状态（track_status）做独热编码，将类别特征转为模型可识别的数值型特征。
5. 收尾清理
   替换无穷值（inf/-inf）为 0，填充剩余缺失值为 0，保证数据无无效值。
   六、标签重构与时序切分（适配任务 + 模拟真实预测）
6. 标签生成（核心翻转）
   将原标签（finish_rank 名次）重构为二分类标签：
   rank_label：finish_rank <= 3 → 1（正样本，前 3 名），其余 →0（负样本），聚焦 “预测前 3 名” 的核心任务。
7. 时序切分（避免时间泄露）
   区别于随机切分，严格按赛事时序切分：
   提取所有唯一 race_id 并排序（保证时间顺序）；
   80% 前的赛事为训练集，20% 后为测试集；
   剔除泄露字段（finish_rank/horse_name/race_id），分离特征（X）和标签（y）。
   七、特征标准化与保存
8. 标准化
   用 StandardScaler 对训练集特征做标准化（均值 0、方差 1），并将相同的缩放规则应用到测试集（避免测试集数据泄露）。
9. 保存
   将处理后的训练 / 测试特征（X）、标签（y）分别保存为 CSV 文件，供后续建模使用。
   预处理核心亮点总结
   时序优先：全程保证 race_id/horse_name 的时序逻辑，切分和历史特征计算均规避时间泄露；
   相对化特征：组内归一化解决不同场次的系统偏差，提升模型泛化能力；
   高效无泄露：用 Expanding Window+shift (1) 替代循环，既提升效率，又杜绝数据泄露；
   稳健性：异常值 clip、分层缺失值填充，保证数据质量；
   任务适配：标签重构为 “前 3 名 = 1”，贴合 AUC 优化目标。

### 模型训练

- 四种模型：LightGBM、XGBoost、CatBoost、tabnet(深度学习)
- 两种结合：blending、stacking
- optuna 调参

### 结果分析

- 模型评估指标：AUC、准确率、F1 分数
- 对比分析：四种模型、blending、stacking 之间的性能差异
- 最优模型选择：根据评估指标选择最优模型
- 模型解释性：分析最优模型的特征重要性，理解模型决策过程
